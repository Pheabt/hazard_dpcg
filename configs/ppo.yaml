# Directory
log_dir: logs
output_dir: outputs
save_dir: models

# Experiment
log_interval: 10
num_eval_episodes: 100

# Environment
distribution_mode: easy
num_levels: 0
start_level: 0
num_env_steps: 1.0e+7
normalize_reward: True

# Rollout
num_processes: 64
num_steps: 256

# Policy gradient
gamma: 0.999
gae_lambda: 0.95

# Actor-critic
actor_critic_class: PPOModel
actor_critic_params:
  shared: True

# Agent
agent_class: PPO
agent_params:
  # PPO params
  clip_param: 0.2
  ppo_epoch: 3
  num_mini_batch: 8
  value_loss_coef: 0.5
  entropy_coef: 0.01
  lr: 5.0e-4
  eps: 1.0e-5
  max_grad_norm: 0.5
